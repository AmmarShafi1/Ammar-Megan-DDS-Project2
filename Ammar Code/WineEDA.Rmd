```{r}
# Load necessary libraries
library(ggplot2)
library(qqplotr)
library(dplyr)
library(corrplot)
library(class)
library(caret)
library(e1071)
```

```{r}
# Load the dataset
wine_data <- read.csv("filtered_combined_data.csv")
```

```{r}
summary(wine_data)
```


```{r}
wine_data[wine_data == ""] <- NA
# Check for missing values
colSums(is.na(wine_data))
```
```{r}
# List of feature columns (excluding 'ID' and 'quality')
features <- setdiff(names(wine_data), c("ID", "quality"))

# Loop through each feature
for (feature in features) {
  # Scatterplot
  scatter_plot <- ggplot(wine_data, aes_string(x = feature, y = "quality")) +
    geom_point(alpha = 0.6) +
    theme_minimal() +
    labs(title = paste("Scatterplot:", feature, "vs Quality"),
         x = feature, y = "Quality")
  print(scatter_plot)
  
  # QQ plot
  qq_plot <- ggplot(wine_data, aes(sample = wine_data[[feature]])) +
    stat_qq_line() +
    stat_qq() +
    theme_minimal() +
    labs(title = paste("QQ Plot:", feature))
  print(qq_plot)
  
  # Boxplot (for the column itself)
  box_plot <- ggplot(wine_data, aes_string(y = feature)) +
    geom_boxplot() +
    theme_minimal() +
    labs(title = paste("Boxplot:", feature),
         y = feature)
  print(box_plot)
}
```

```{r}
# Correlation matrix and heatmap
cor_matrix <- cor(wine_data[ , sapply(wine_data, is.numeric)])
corrplot(cor_matrix, method = "color", addCoef.col = "black", number.cex = 0.7,
         tl.col = "black", tl.srt = 45, title = "Correlation Heatmap")

# Distribution of quality
ggplot(wine_data, aes(x = quality)) +
  geom_bar(fill = "steelblue") +
  ggtitle("Distribution of Wine Quality") +
  xlab("Wine Quality") +
  ylab("Count")
```
```{r}

# Select relevant columns for the KNN model
features <- wine_data[, c("volatile.acidity", "sulphates", "density", "alcohol")]
target <- wine_data$quality

# Normalize the feature columns
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}
features <- as.data.frame(lapply(features, normalize))

# Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- createDataPartition(target, p = 0.7, list = FALSE)
train_features <- features[train_indices, ]
test_features <- features[-train_indices, ]
train_target <- target[train_indices]
test_target <- target[-train_indices]

# Ensure that the levels of the training and testing target are consistent
train_target <- factor(train_target, levels = sort(unique(target)))
test_target <- factor(test_target, levels = sort(unique(target)))

# Evaluate KNN for k values from 1 to 10
results <- data.frame(k = integer(), Accuracy = numeric(), Kappa = numeric(), RMSE = numeric())

for (k in 1:10) {
  # Fit the KNN model
  knn_pred <- knn(train = train_features, test = test_features, cl = train_target, k = k)
  
  # Convert predictions to numeric for RMSE calculation
  knn_pred_numeric <- as.numeric(as.character(knn_pred))
  test_target_numeric <- as.numeric(as.character(test_target))
  
  # Calculate RMSE
  rmse <- sqrt(mean((test_target_numeric - knn_pred_numeric)^2))
  
  # Convert predictions to factor with the same levels as the target
  knn_pred <- factor(knn_pred, levels = levels(test_target))
  
  # Evaluate the model
  confusion <- confusionMatrix(knn_pred, test_target)
  accuracy <- confusion$overall["Accuracy"]
  kappa <- confusion$overall["Kappa"]
  
  # Save the results
  results <- rbind(results, data.frame(k = k, Accuracy = accuracy, Kappa = kappa, RMSE = rmse))
}

# Find the best k value based on Accuracy
best_k <- results[which.max(results$Accuracy), ]

# Print the results
print(results)
cat("\nBest k value based on Accuracy:\n")
print(best_k)
```

```{r}
# Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- createDataPartition(target, p = 0.8, list = FALSE)
train_features <- features[train_indices, ]
test_features <- features[-train_indices, ]
train_target <- target[train_indices]
test_target <- target[-train_indices]

# Ensure the target variable is treated as a factor with consistent levels
train_target <- factor(train_target, levels = sort(unique(target)))
test_target <- factor(test_target, levels = sort(unique(target)))

# Train the Naive Bayes model
nb_model <- naiveBayes(train_features, train_target)

# Predict on the test data
nb_predictions <- predict(nb_model, test_features)

# Ensure predictions are factors with the same levels as test_target
nb_predictions <- factor(nb_predictions, levels = levels(test_target))

# Convert predictions to numeric for RMSE calculation
nb_predictions_numeric <- as.numeric(as.character(nb_predictions))
test_target_numeric <- as.numeric(as.character(test_target))

# Calculate RMSE
rmse <- sqrt(mean((test_target_numeric - nb_predictions_numeric)^2))

# Evaluate the model
confusion <- confusionMatrix(nb_predictions, test_target)
accuracy <- confusion$overall["Accuracy"]
kappa <- confusion$overall["Kappa"]

# Print results
cat("Naive Bayes Model Performance:\n")
cat("Accuracy:", accuracy, "\n")
cat("Kappa:", kappa, "\n")
cat("RMSE:", rmse, "\n")
```

```{r}
#Alcohol vs quality (increasing), total sulfur dioxide vs quality (decreasing)
#Sweetness: residual sugars vs sulfur dioxide, density
#density is strongly correlated with residual sugar(positive) and very negatively correlated with alcohol
#therefore, the sweeter the alcohol, the less alcoholic it is and vis versa
#Cirtic Acid is highly correlated with fixed and volatile acidity

```

```{r}
# 1. Alcohol vs Quality (Increasing Trend)
ggplot(wine_data, aes(x = alcohol, y = quality)) +
  geom_point(alpha = 0.6, color = "blue") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  theme_minimal() +
  labs(title = "Alcohol vs Quality", x = "Alcohol", y = "Quality")

# 2. Total Sulfur Dioxide vs Quality (Decreasing Trend)
ggplot(wine_data, aes(x = total.sulfur.dioxide, y = quality)) +
  geom_point(alpha = 0.6, color = "green") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  theme_minimal() +
  labs(title = "Total Sulfur Dioxide vs Quality", x = "Total Sulfur Dioxide", y = "Quality")

# 3. Residual Sugar vs Sulfur Dioxide
ggplot(wine_data, aes(x = residual.sugar, y = total.sulfur.dioxide)) +
  geom_point(alpha = 0.6, color = "purple") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  theme_minimal() +
  labs(title = "Residual Sugar vs Total Sulfur Dioxide", x = "Residual Sugar", y = "Total Sulfur Dioxide")

# 4. Residual Sugar vs Density
ggplot(wine_data, aes(x = residual.sugar, y = density)) +
  geom_point(alpha = 0.6, color = "orange") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  theme_minimal() +
  labs(title = "Residual Sugar vs Density", x = "Residual Sugar", y = "Density")

# 5. Density vs Alcohol (Negative Correlation)
ggplot(wine_data, aes(x = density, y = alcohol)) +
  geom_point(alpha = 0.6, color = "darkred") +
  geom_smooth(method = "lm", color = "blue", se = FALSE) +
  theme_minimal() +
  labs(title = "Density vs Alcohol", x = "Density", y = "Alcohol")

# 6. Citric Acid vs Fixed Acidity
ggplot(wine_data, aes(x = citric.acid, y = fixed.acidity)) +
  geom_point(alpha = 0.6, color = "cyan") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  theme_minimal() +
  labs(title = "Citric Acid vs Fixed Acidity", x = "Citric Acid", y = "Fixed Acidity")

# 7. Citric Acid vs Volatile Acidity
ggplot(wine_data, aes(x = citric.acid, y = volatile.acidity)) +
  geom_point(alpha = 0.6, color = "brown") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  theme_minimal() +
  labs(title = "Citric Acid vs Volatile Acidity", x = "Citric Acid", y = "Volatile Acidity")
```



